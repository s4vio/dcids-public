{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe717fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from keras import optimizers, Sequential\n",
    "from keras.models import Model, Sequential, save_model, load_model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Conv1D\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "from ipynb.fs.full.rcids_functions import *\n",
    "\n",
    "np.set_printoptions(suppress=True) #prevent numpy exponential\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x) #prevent scientific notation in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23297f5b",
   "metadata": {},
   "source": [
    "## Reading from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading existent df from disk\n",
    "df_benign_data = pd.read_pickle(\"pkl/df_proc_benign_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e21806",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa91b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining window_size, n_feature and normalization function\n",
    "window_size = 6\n",
    "n_features = df_benign_data.shape[1]\n",
    "norm_function = \"mm\"                 # std (StandardScaler), norm (Normalizer), mm (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d84148",
   "metadata": {},
   "source": [
    "### Splitting Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data, df_test_data = train_test_split(df_benign_data, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ad28c",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b81233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "mm = MinMaxScaler()\n",
    "mm_train = mm.fit(df_train_data)\n",
    "train_data = mm.transform(df_train_data)\n",
    "\n",
    "print(\"Train data numpy.ndarray shape:\", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3D array for train data\n",
    "# For an LSTM Autoencoder the shape of input has to be of the format: n_samples x window_size x n_features\n",
    "train_data_wz = pd.DataFrame(train_data)\n",
    "train_data_wz = sliding_window(train_data_wz, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987271e",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "mm = MinMaxScaler()\n",
    "mm_test = mm.fit(df_train_data) # Fit deve ser feito com dados de treinanento\n",
    "test_data = mm_test.transform(df_test_data) # Apenas transform nos dados de teste\n",
    "\n",
    "print(\"Test data numpy.ndarray shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3D array for test data\n",
    "# For an LSTM Autoencoder the shape of input has to be of the format: n_samples x window_size x n_features\n",
    "test_data_wz = pd.DataFrame(test_data)\n",
    "test_data_wz = sliding_window(test_data_wz, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e17f96",
   "metadata": {},
   "source": [
    "## Creating Tensorflow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54db3f",
   "metadata": {},
   "source": [
    "### Trainning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17098ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(train_data_wz)\n",
    "ds_train = ds_train.map(lambda x: (x, x))\n",
    "ds_train_batch = ds_train.batch(1024).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8047089",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72567247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(test_data_wz)\n",
    "ds_test = ds_test.map(lambda x: (x, x))\n",
    "ds_test_batch = ds_test.batch(1024).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5028cf5",
   "metadata": {},
   "source": [
    "## Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d517f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperopt = pd.DataFrame(columns=[\"Model\",\"Params\",\"Max_Loss\", \"Loss_99\"])\n",
    "index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "            \n",
    "    model = Sequential()\n",
    "\n",
    "    # Conv1D\n",
    "    model.add(keras.layers.Conv1D(filters=space['filters'], kernel_size=space['kernel_size'], strides=1, padding=\"same\", activation=\"relu\", input_shape=(window_size, n_features)))\n",
    "\n",
    "    # Encoder\n",
    "    #model.add(CuDNNLSTM(space['first'], kernel_initializer=space['kernel_init'], input_shape=(window_size, n_features), return_sequences=True))\n",
    "    model.add(CuDNNLSTM(space['first'], kernel_initializer=space['kernel_init'], return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "         \n",
    "    model.add(CuDNNLSTM(space['second'], kernel_initializer=space['kernel_init'], return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "         \n",
    "    model.add(CuDNNLSTM(space['third'], kernel_initializer=space['kernel_init'], return_sequences=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "\n",
    "    model.add(RepeatVector(window_size))\n",
    "\n",
    "    # Decoder\n",
    "    model.add(CuDNNLSTM(space['third'], kernel_initializer=space['kernel_init'], return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "\n",
    "    model.add(CuDNNLSTM(space['second'], kernel_initializer=space['kernel_init'], return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "    \n",
    "    model.add(CuDNNLSTM(space['first'], kernel_initializer=space['kernel_init'], return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(space['activ']))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "        \n",
    "    model.compile(loss=space['loss_ob'], optimizer=space['optimizer'], metrics=space['metrics'])\n",
    "\n",
    "    history = model.fit(ds_train_batch, epochs=space['epochs'], shuffle=False, verbose=0)\n",
    "    \n",
    "    # Predicting values using the trained model\n",
    "    pred = model.predict(ds_test_batch)\n",
    "\n",
    "    # Creating dataframes for loss calc\n",
    "    # Reshaping array with predictions to 2D dataframe (column 2 x column 3)\n",
    "    #X_pred.shape #--> (samples - window_size, window_size, n_features)\n",
    "    pred = pred.reshape(pred.shape[0], pred.shape[1] * pred.shape[2])\n",
    "    df_pred = pd.DataFrame(pred)\n",
    "    \n",
    "    # Reshaping array with real data to 2D dataframe (column 2 x column 3)\n",
    "    test = test_data_wz.reshape(test_data_wz.shape[0], test_data_wz.shape[1] * test_data_wz.shape[2])\n",
    "    df_test = pd.DataFrame(test)\n",
    "    \n",
    "    # Calculating test loss with MAE (Mean Absolute Error)\n",
    "    df_test_loss = pd.DataFrame(index=df_pred.index)\n",
    "    df_test_loss['Loss_mae'] = tf.metrics.MAE(df_test, df_pred)\n",
    "    loss_threshold_max = np.round(df_test_loss.values.max(), 4)\n",
    "\n",
    "    # Loss threshold = 99% percentile of loss in test data\n",
    "    loss_threshold_99 = np.percentile(df_test_loss['Loss_mae'].values, 99)\n",
    "      \n",
    "    # Creating dataframe with metrics\n",
    "    global index\n",
    "    df_hyperopt.loc[index,:]=[index, space, loss_threshold_max, loss_threshold_99]\n",
    "\n",
    "    # Saving df to disk\n",
    "    df_hyperopt.to_pickle('pkl/df_hyperopt.pkl')\n",
    "    \n",
    "    index=index+1\n",
    "    \n",
    "    print(space, loss_threshold_max)\n",
    "    \n",
    "    return {'status': STATUS_OK, 'loss': loss_threshold_99, 'Params': space}\n",
    "\n",
    "\n",
    "space ={'filters': hp.choice('filters', np.arange(1, 10, 1)),\n",
    "'kernel_size': hp.choice('kernel_size', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]),\n",
    "'first': hp.choice('first', np.arange(96, 192, 8)),\n",
    "'second': hp.choice('second', np.arange(64, 96, 4)),\n",
    "'third': hp.choice('third', np.arange(16, 32, 2)),\n",
    "'kernel_init' : hp.choice('kernel_init', [\"he_normal\"]),\n",
    "'activ' : hp.choice('activ', [\"tanh\"]),\n",
    "#'dropout' : hp.choice('dropout', [0.0, 0.05, 0.1, 0.15, 0.2]),\n",
    "'loss_ob' : hp.choice('loss_ob', [\"mae\"]),\n",
    "'optimizer' : hp.choice('optimizer', [\"nadam\"]),\n",
    "'metrics' : hp.choice('metrics', [\"accuracy\"]),\n",
    "'epochs' : hp.choice('epochs', [10])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cffb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=300,\n",
    "            trials=trials, \n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperopt.shape, df_hyperopt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.DataFrame(trials.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd6354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to pickle file\n",
    "df_trials.to_pickle('pkl/df_trials_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae35c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperopt.sort_values(['Loss_99'], ascending=[True])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec  7 2022, 00:00:00) [GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
