{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe717fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) #prevent numpy exponential\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "#from keras import optimizers, Sequential, metrics\n",
    "from elasticsearch import Elasticsearch\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Conv1D\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from ipynb.fs.full.rcids_functions import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(suppress=True) #prevent numpy exponential\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x) #prevent scientific notation in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb37f87",
   "metadata": {},
   "source": [
    "### Reading from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d62713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conectivity with Elasticsearch\n",
    "es = Elasticsearch(host=\"192.168.201.2\", http_auth=(\"elastic\",\"##redacted##\"))\n",
    "es.info(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining elasticsearch indice to read from\n",
    "index = \"proc-public-benign\"\n",
    "\n",
    "# Counting number of documents in index\n",
    "n_docs = es.count(index=index)\n",
    "print(\"Number of documents in the index\", index, \"-->\", n_docs['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c70f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset for trainning\n",
    "df_benign_data = read_from_elastic(index, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding timestamp column\n",
    "df_benign_data.drop(['timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252d4f8",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5dc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining window_size, n_feature and normalization function\n",
    "window_size = 6\n",
    "n_features = df_benign_data.shape[1]\n",
    "norm_function = \"mm\"                 # std (StandardScaler), norm (Normalizer), mm (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892e7be",
   "metadata": {},
   "source": [
    "### Benign HashDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e8a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashdb_name = \"df_proc_benign_hashdb_wz\" + str(window_size) + \"_ft\" + str(n_features) + \"_\" + str(norm_function) + \".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"pkl/\" + hashdb_name): \n",
    "    # Loading existent df from disk\n",
    "    df_benign_hashdb = pd.read_pickle(\"pkl/\" + hashdb_name)\n",
    "else:\n",
    "    # Normalizing data\n",
    "    #norm = Normalizer()\n",
    "    #norm_benign = norm.fit(df_benign_data)\n",
    "    #benign_data = norm_benign.transform(df_benign_data)\n",
    "\n",
    "    mm = MinMaxScaler()\n",
    "    mm_benign = mm.fit(df_benign_data)\n",
    "    benign_data = mm_benign.transform(df_benign_data)\n",
    "\n",
    "    # Creating 3D array for train data\n",
    "    # For an LSTM Autoencoder the shape of input has to be of the format: n_samples x window_size x n_features\n",
    "    benign_data_wz = pd.DataFrame(benign_data)\n",
    "    benign_data_wz = sliding_window(benign_data_wz, window_size)\n",
    "\n",
    "    # Coverting to 2d pandas df\n",
    "    benign_data_wz_2d = benign_data_wz.reshape(benign_data_wz.shape[0], benign_data_wz.shape[1] * benign_data_wz.shape[2])\n",
    "    df_benign_data_wz_2d = pd.DataFrame(benign_data_wz_2d)\n",
    "    # Calculating rows (windows) hash\n",
    "    df_benign_data_hash = df_benign_data_wz_2d.apply(lambda x: hash(tuple(x)).to_bytes(8, \"big\", signed=True).hex(), axis=1)\n",
    "    # Removing duplicates\n",
    "    df_benign_hashdb = pd.DataFrame(df_benign_data_hash.unique())\n",
    "    # Saving df to disk\n",
    "    df_benign_hashdb.to_pickle(\"pkl/\" + hashdb_name)\n",
    "\n",
    "print(hashdb_name + \" size: \")\n",
    "df_benign_hashdb.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48347141",
   "metadata": {},
   "source": [
    "## Splitting Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bca199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data, df_test_data = train_test_split(df_benign_data, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e21806",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bb490",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b81233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "#norm = Normalizer()\n",
    "#norm_train = norm.fit(df_train_data)\n",
    "#train_data = norm_train.transform(df_train_data)\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "mm_train = mm.fit(df_train_data)\n",
    "train_data = mm.transform(df_train_data)\n",
    "\n",
    "print(\"Train data numpy.ndarray shape:\", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3D array for train data\n",
    "# For an LSTM Autoencoder the shape of input has to be of the format: n_samples x window_size x n_features\n",
    "train_data_wz = pd.DataFrame(train_data)\n",
    "train_data_wz = sliding_window(train_data_wz, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e17f96",
   "metadata": {},
   "source": [
    "## Creating Tensorflow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65c8c6",
   "metadata": {},
   "source": [
    "### Trainning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17098ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "ds_train_full = tf.data.Dataset.from_tensor_slices(train_data_wz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1145cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train_full.take(0.95 * ds_train_full.cardinality().numpy())\n",
    "ds_validation = ds_train_full.take(0.05 * ds_train_full.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890237bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(lambda x: (x, x))\n",
    "ds_train_batch = ds_train.batch(1024).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_validation = ds_validation.map(lambda x: (x, x))\n",
    "ds_validation_batch = ds_validation.batch(1024).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d2bda",
   "metadata": {},
   "source": [
    "## Defining and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66165588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tfds_lstm_160_64_24_conv1d_relu_5_bn_tahn_wz' + str(window_size) + '_ft' + str(n_features) + '_' + str(norm_function)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "# Conv1D\n",
    "model.add(keras.layers.Conv1D(filters=n_features, kernel_size=window_size, strides=1, padding=\"same\", activation=\"relu\", input_shape=(window_size, n_features)))\n",
    "\n",
    "# Encoder\n",
    "model.add(CuDNNLSTM(160, kernel_initializer='he_normal', return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "         \n",
    "model.add(CuDNNLSTM(64, kernel_initializer='he_normal', return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "model.add(CuDNNLSTM(24, kernel_initializer='he_normal', return_sequences=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "model.add(RepeatVector(window_size))\n",
    "\n",
    "# Decoder\n",
    "model.add(CuDNNLSTM(24, kernel_initializer='he_normal', return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "model.add(CuDNNLSTM(64, kernel_initializer='he_normal', return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "\n",
    "model.add(CuDNNLSTM(160, kernel_initializer='he_normal', return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('tanh'))\n",
    "    \n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "       \n",
    "model.compile(loss='mae', optimizer='nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning parameters\n",
    "train_log = CSVLogger('models/log-' + str(model_name) + '.log', separator=',', append=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.001, verbose=1)\n",
    "learning_rate = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='model-' + str(model_name) + '.h5', monitor='val_loss', mode='min', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01444c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(ds_train_batch, epochs=100, shuffle=False, callbacks=[train_log, early_stopping, learning_rate, mc], validation_data=ds_validation_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving/Loading the model\n",
    "filepath = 'models/model-' + str(model_name) + '.h5'\n",
    "#save_model(model, filepath)\n",
    "model = load_model(filepath, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading model training history \n",
    "df_history = pd.read_csv('models/log-' + str(model_name) + '.log', sep=',', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be227c",
   "metadata": {},
   "source": [
    "## Loss distribution for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss distribution\n",
    "plot = sns.displot(data=df_history['val_loss'], kind='kde', color='blue', height=5, aspect=2)\n",
    "plot.set_axis_labels(\"Validation Loss\", \"Density\")\n",
    "plot.set(title='Training Validation Loss Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7354d",
   "metadata": {},
   "source": [
    "## Defining the Loss Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bb490",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b81233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "#norm = Normalizer()\n",
    "#norm_test = norm.fit(df_train_data) # Fit deve ser feito com dados de treinanento\n",
    "#test_data = norm_test.transform(df_test_data) # Aplicar transform nos dados de teste apÃ³s o fit\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "mm_test = mm.fit(df_train_data) # Fit deve ser feito com dados de treinanento\n",
    "test_data = mm_test.transform(df_test_data) # Apenas transform nos dados de teste\n",
    "\n",
    "print(\"Test data numpy.ndarray shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3D array for train data\n",
    "# For an LSTM Autoencoder the shape of input has to be of the format: n_samples x window_size x n_featuress\n",
    "test_data_wz = pd.DataFrame(test_data)\n",
    "test_data_wz = sliding_window(test_data_wz, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65c8c6",
   "metadata": {},
   "source": [
    "### Tensorflow test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17098ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(test_data_wz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890237bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(lambda x: (x, x))\n",
    "ds_test_batch = ds_test.batch(1024).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3863f32",
   "metadata": {},
   "source": [
    "### Predicting test data using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dae0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values using the trained model\n",
    "pred = model.predict(ds_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping array with predictions to 2D dataframe (column 2 x column 3)\n",
    "#X_pred.shape #--> (samples - window_size, window_size, n_features)\n",
    "pred = pred.reshape(pred.shape[0], pred.shape[1] * pred.shape[2])\n",
    "df_pred = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef465774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping array with real data to 2D dataframe (column 2 x column 3)\n",
    "#X_test.shape # --> (samples - window_size, window_size, n_features)\n",
    "test = test_data_wz.reshape(test_data_wz.shape[0], test_data_wz.shape[1] * test_data_wz.shape[2])\n",
    "df_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ce94c",
   "metadata": {},
   "source": [
    "### Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating test loss with MAE (Mean Absolute Error)\n",
    "df_test_loss = pd.DataFrame(index=df_pred.index)\n",
    "df_test_loss['Loss_mae'] = tf.metrics.MAE(df_test, df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac840c46",
   "metadata": {},
   "source": [
    "### Defining the loss threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_loss['Loss_mae'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining threshold based on the training loss\n",
    "#loss_threshold = np.round(df_train_loss.quantile([.75]).values[0][0], 4)\n",
    "df_test_loss_mean = df_test_loss['Loss_mae'].values.mean()\n",
    "df_test_loss_std = df_test_loss['Loss_mae'].values.std()\n",
    "loss_threshold_mean_std = np.round(df_test_loss_mean + df_test_loss_std, 4)\n",
    "loss_threshold_max = np.round(df_test_loss.values.max(), 4)\n",
    "loss_threshold_percentile = np.round(np.percentile(df_test_loss['Loss_mae'].values, 99), 4)\n",
    "print(\"Threshold based on the max loss during the tests --> \", loss_threshold_max)\n",
    "print(\"Threshold calculated through the mean + std deviation --> \", \n",
    "loss_threshold_mean_std)\n",
    "print(\"Threshold basead on the 99 percentil of loss during the tests --> \", loss_threshold_percentile)\n",
    "\n",
    "thresholds = loss_threshold_mean_std, loss_threshold_max, loss_threshold_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining confidence levels for threshold adjustment\n",
    "confidence_levels =  [1, 0.995, 0.99, 0.98, 0.97, 0.96, 0.95]\n",
    "\n",
    "# Calling function and creating dataframe with thresholds\n",
    "thresholds = tunable_threshold(df_test_loss, confidence_levels)\n",
    "\n",
    "# Printing threshold per confidence level\n",
    "print(\"--- Threshold for each confidence interval ---\")\n",
    "for i in confidence_levels:\n",
    "    print(\"Confidence Interval [\", i, \"] --> \", thresholds.iloc[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model-' + str(model_name) + '.h5' \n",
    "\n",
    "df_name = 'df_proc_thresholds.pkl'\n",
    "\n",
    "if os.path.isfile('pkl/' + df_name):\n",
    "    # Loading existent df from disk\n",
    "    df_thresholds = pd.read_pickle('pkl/' + df_name)\n",
    "\n",
    "    for i in confidence_levels:\n",
    "        # Adding last execution results in to dataframe\n",
    "        df_thresholds.loc[df_thresholds.shape[0]] = [filepath, i, thresholds.iloc[0][i]]\n",
    "        \n",
    "    # Saving df to disk\n",
    "    df_thresholds.to_pickle('pkl/' + df_name)\n",
    "\n",
    "else:\n",
    "    # Defining dataframe columns\n",
    "    df_thresholds = pd.DataFrame(columns=[\"Model\", \"Confidence_Level\", \"Threshold\"])\n",
    "\n",
    "    for i in confidence_levels:\n",
    "    # Adding last execution results in to dataframe\n",
    "        df_thresholds.loc[df_thresholds.shape[0]] = [filepath, i, thresholds.iloc[0][i]]\n",
    "\n",
    "    # Saving df to disk\n",
    "    df_thresholds.to_pickle('pkl/' + df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds.sort_values(['Model']).groupby(['Model']).value_counts()\n",
    "#df_thresholds.sort_values(['Confidence_Level'], ascending=(False)).groupby(['Model']).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_threshold = float(input(\"Choose one of the thresholds: \\n{}\".format(thresholds.to_string(header=None, index=False))))\n",
    "print(\"Chosen threshold --> \", loss_threshold)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
